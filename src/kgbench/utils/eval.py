import re
from typing import Any, Dict, List, Set

from kgbench.utils.nlp import stem, tokenize


def calculate_precision_recall_f1(gold: Set, pred: Set) -> (float, float, float):
    """
    Method to calculate precision, recall and f1:
        Precision is calculated as correct_triples/predicted_triples and
        Recall as correct_triples/gold_triples
        F1 as the harmonic mean of precision and recall.
    :param gold: items in the gold standard
    :param pred: items in the system prediction
    :return:
        p: float - precision
        r: float - recall
        f1: float - F1
    """
    if len(pred) == 0:
        return 0, 0, 0
    p = len(gold.intersection(pred)) / len(pred)
    r = len(gold.intersection(pred)) / len(gold)
    if p + r > 0:
        f1 = 2 * ((p * r) / (p + r))
    else:
        f1 = 0
    return p, r, f1


def get_subject_object_hallucinations(
        ontology, test_sentence, triples) -> (float, float):
    """
    Calculate subject and object hallucinations metrics. As the context for calculating hallucinations, we consider the
    test sentence and the ontology concepts as relevant tokens.
    :param ontology: ontology to take into account with the concepts and relations
    :param test_sentence: test sentences for which the triples are generated
    :param triples: a set of triples generated by the system
    :return:
        subj_hallucination: float - subject hallucination metric
        obj_hallucination: float - object hallucination metric
    """

    # if the set of triples are empty, we return 0
    if len(triples) == 0:
        return 0, 0

    # append the test sentence with concepts from the ontology
    test_sentence += " ".join([c["label"] for c in ontology["concepts"]])
    # stem each word in the test sentence concatenated with the ontology concepts
    stemmed_sentence = "".join([stem(word) for word in tokenize(test_sentence)])
    # normalize the text to remove white spaces and underscores
    normalized_stemmed_sentence = re.sub(r"(_|\s+)", "", stemmed_sentence).lower()

    # count the number of subject and object hallucinations
    num_subj_hallucinations, num_obj_hallucinations = 0, 0
    for triple in triples:
        # clean and normalize subject and object noun phrases the same way as the test sentence
        normalized_stemmed_subject = clean_entity_string(triple[0])
        normalized_stemmed_object = clean_entity_string(triple[2])

        # check if the subject/object is found in the stemmed sentence/context text. If not found, mark it as a hallucination
        if normalized_stemmed_sentence.find(normalized_stemmed_subject) == -1:
            num_subj_hallucinations += 1
        if normalized_stemmed_sentence.find(normalized_stemmed_object) == -1:
            num_obj_hallucinations += 1

    # divide the number of hallucinations by the number of triples to calculate the hallucination metrics
    subj_hallucination = num_subj_hallucinations / len(triples)
    obj_hallucination = num_obj_hallucinations / len(triples)
    return subj_hallucination, obj_hallucination


def get_ontology_conformance(ontology: Dict, triples: List) -> (float, float):
    """
    Calculate the ontology conformance and relation hallucination metrics.
    :param ontology: ontology to take into account with the concepts and relations
    :param triples: a set of triples generated by the system
    :return:
        ont_conformance: float - ontology conformance metric
        rel_hallucination: float - relation hallucination metric = 1 - ontology conformance
    """
    if len(triples) == 0:
        return 1, 0
    # replace spaces with underscores in the ontology relations
    ont_rels = [rel["label"].replace(" ", "_") for rel in ontology["relations"]]
    # count the number of system triples relations that are in the ontology
    num_rels_conformant = len([tr for tr in triples if tr[1] in ont_rels])

    # ontology conformance is the number of system triples relations in the ontology divided by the total number of system triples
    ont_conformance = num_rels_conformant / len(triples)
    # relation hallucination is 1 - ontology conformance
    rel_hallucination = 1 - ont_conformance
    return ont_conformance, rel_hallucination


def normalize_triple(sub_label: str, rel_label: str, obj_label: str) -> str:
    """
    Normalize triples for comparison in precision, recall calculations
    :param sub_label: subject string
    :param rel_label: relation string
    :param obj_label: object string
    :return: a normalized triple as a single concatenated string
    """
    # remove spaces and underscores and make lower case
    sub_label = stem(re.sub(r"(_|\s+)", "", sub_label).lower())
    rel_label = stem(re.sub(r"(_|\s+)", "", rel_label).lower())
    obj_label = stem(re.sub(r"(_|\s+)", "", obj_label).lower())
    # concatenate them to a single string
    tr_key = f"{sub_label}{rel_label}{obj_label}"
    print(tr_key)
    return tr_key


def clean_entity_string(entity: str) -> str:
    """
    Utility method to clean subject and object strings of triples
    :param entity: subject or object string
    :return: the cleaned and normalized string
    """
    # stem every word for better matches
    stemmed_entity = "".join([stem(word) for word in tokenize(entity)])
    # normalizing the string by removing white spaces, underscores and then converting to lower case
    normalized_stemmed_entity = re.sub(r"(_|\s+)", "", stemmed_entity).lower()
    # special handling for string with years to remove January 01
    return normalized_stemmed_entity.replace("01januari", "")


def convert_to_dict(data: List[Dict], id_name: str = "id") -> Dict:
    """
    Utility method to convert a list to a dictionary
    :param data: a list of dictionary objects
    :param id_name: the attribute to be used as the key for the dictionary
    :return: a dictionary with the same content as the list
    """
    return {item[id_name]: item for item in data}


def calculate_metrics(
    start_time: float, end_time: float, response: str, prompt: str
) -> Dict[str, Any]:
    total_time = end_time - start_time
    response_tokens = len(response.split())
    prompt_tokens = len(prompt.split())
    total_tokens = response_tokens + prompt_tokens

    metrics = {
        "total_time_seconds": round(total_time, 3),
        "tokens_per_second": round(response_tokens / total_time, 2),
        "prompt_tokens": prompt_tokens,
        "response_tokens": response_tokens,
        "total_tokens": total_tokens,
        "response_length_chars": len(response),
        "memory_usage_mb": None,  # Will be filled by GPU memory usage
    }
    return metrics


def print_metrics(result: Dict[str, Any]):
    if not result["success"]:
        print(f"\nError: {result['error']}")
        return

    print("\nResponse:", result["response"])
    print("\nPerformance Metrics:")
    print("-" * 50)
    metrics = result["metrics"]
    print(f"Total Time: {metrics['total_time_seconds']:.3f} seconds")
    print(f"Tokens Per Second: {metrics['tokens_per_second']:.2f}")
    print(f"Prompt Tokens: {metrics['prompt_tokens']}")
    print(f"Response Tokens: {metrics['response_tokens']}")
    print(f"Total Tokens: {metrics['total_tokens']}")
    print(f"Response Length: {metrics['response_length_chars']} characters")
    if metrics["memory_usage_mb"]:
        print(f"GPU Memory Usage: {metrics['memory_usage_mb']} MB")
