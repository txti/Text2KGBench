{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontologies(ontology_dir):\n",
    "    print(\"Loading ontologies...\")\n",
    "    ontologies = []\n",
    "    for file_name in os.listdir(ontology_dir):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(ontology_dir, file_name), 'r') as f:\n",
    "                ont = json.load(f)\n",
    "                ontologies.append(ont)\n",
    "                print(f\"Loaded {ont['title']} ({ont['id']})\")\n",
    "    return ontologies\n",
    "\n",
    "def process_field(value):\n",
    "    \"\"\"Process TSV field values and split multi-values\"\"\"\n",
    "    if not value or value.strip() in ['-', '']:\n",
    "        return []\n",
    "    return [v.strip() for v in value.split('|')]\n",
    "\n",
    "def generate_triples(row, relation):\n",
    "    \"\"\"Generate triples for a given relation considering multi-value fields\"\"\"\n",
    "    domain_field = relation['domain']\n",
    "    range_field = relation['range']\n",
    "    rel_label = relation['label']\n",
    "    \n",
    "    domain_values = process_field(row.get(domain_field, ''))\n",
    "    range_values = process_field(row.get(range_field, ''))\n",
    "    \n",
    "    triples = []\n",
    "    \n",
    "    if not domain_values or not range_values:\n",
    "        return triples\n",
    "    \n",
    "    # Handle different length combinations\n",
    "    len_domain = len(domain_values)\n",
    "    len_range = len(range_values)\n",
    "    \n",
    "    if len_domain == len_range:\n",
    "        for d, r in zip(domain_values, range_values):\n",
    "            triples.append({'sub': d, 'rel': rel_label, 'obj': r})\n",
    "    elif len_domain > 1 and len_range == 1:\n",
    "        for d in domain_values:\n",
    "            triples.append({'sub': d, 'rel': rel_label, 'obj': range_values[0]})\n",
    "    elif len_range > 1 and len_domain == 1:\n",
    "        for r in range_values:\n",
    "            triples.append({'sub': domain_values[0], 'rel': rel_label, 'obj': r})\n",
    "    else:\n",
    "        for i in range(min(len_domain, len_range)):\n",
    "            triples.append({'sub': domain_values[i], 'rel': rel_label, 'obj': range_values[i]})\n",
    "    \n",
    "    return triples\n",
    "\n",
    "def process_tsv(tsv_path, ontologies):\n",
    "    print(f\"\\nProcessing TSV file: {tsv_path}\")\n",
    "    ontology_data = defaultdict(lambda: {'sents': []})\n",
    "    \n",
    "    with open(tsv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        total_rows = 0\n",
    "        for row in reader:\n",
    "            total_rows += 1\n",
    "            sentence = row.get('Sentence', '').strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            \n",
    "            for ontology in ontologies:\n",
    "                ontology_triples = []\n",
    "                for relation in ontology['relations']:\n",
    "                    ontology_triples.extend(generate_triples(row, relation))\n",
    "                \n",
    "                if ontology_triples:\n",
    "                    ontology_id = ontology['id']\n",
    "                    ontology_data[ontology_id]['sents'].append({\n",
    "                        'sent': sentence,\n",
    "                        'triples': ontology_triples\n",
    "                    })\n",
    "        \n",
    "        print(f\"Processed {total_rows} TSV rows\")\n",
    "    return ontology_data\n",
    "\n",
    "def split_and_save_data(ontology_data):\n",
    "    print(\"\\nSplitting data and saving files:\")\n",
    "    total_all_sents = 0\n",
    "    total_train = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    for ontology_id, data in ontology_data.items():\n",
    "        sents = data['sents']\n",
    "        all_subjects = set()\n",
    "        \n",
    "        # Collect all unique subjects\n",
    "        for sent in sents:\n",
    "            for triple in sent['triples']:\n",
    "                all_subjects.add(triple['sub'])\n",
    "        \n",
    "        # Split subjects into train/test\n",
    "        all_subjects = list(all_subjects)\n",
    "        np.random.shuffle(all_subjects)\n",
    "        split_idx = int(0.8 * len(all_subjects))\n",
    "        train_subjects = set(all_subjects[:split_idx])\n",
    "        test_subjects = set(all_subjects[split_idx:])\n",
    "        \n",
    "        # Split sentences\n",
    "        train_sents = []\n",
    "        test_sents = []\n",
    "        for sent in sents:\n",
    "            sent_subjects = {triple['sub'] for triple in sent['triples']}\n",
    "            if sent_subjects & test_subjects:\n",
    "                test_sents.append(sent)\n",
    "            else:\n",
    "                train_sents.append(sent)\n",
    "        \n",
    "        # Print statistics for this ontology\n",
    "        total = len(sents)\n",
    "        train_count = len(train_sents)\n",
    "        test_count = len(test_sents)\n",
    "        print(f\"\\n{ontology_id}:\")\n",
    "        print(f\"  Total sentences: {total}\")\n",
    "        print(f\"  Train sentences: {train_count}\")\n",
    "        print(f\"  Test sentences: {test_count}\")\n",
    "        print(f\"  Unique subjects: {len(all_subjects)}\")\n",
    "        \n",
    "        # Update global totals\n",
    "        total_all_sents += total\n",
    "        total_train += train_count\n",
    "        total_test += test_count\n",
    "        \n",
    "        # Save datasets\n",
    "        os.makedirs('./data/odeuropa/train', exist_ok=True)\n",
    "        os.makedirs('./data/odeuropa/test', exist_ok=True)\n",
    "        os.makedirs('./data/odeuropa/ground_truth', exist_ok=True)\n",
    "        \n",
    "        # Save training data\n",
    "        train_path = f'./data/odeuropa/train/{ontology_id}_train.jsonl'\n",
    "        with open(train_path, 'w') as f:\n",
    "            for idx, sent in enumerate(train_sents, 1):\n",
    "                entry = {\n",
    "                    'id': f'{ontology_id}_train_{idx}',\n",
    "                    'sent': sent['sent'],\n",
    "                    'triples': sent['triples']\n",
    "                }\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "        \n",
    "        # Save test and ground truth\n",
    "        test_path = f'./data/odeuropa/test/{ontology_id}_test.jsonl'\n",
    "        gt_path = f'./data/odeuropa/ground_truth/{ontology_id}_ground_truth.jsonl'\n",
    "        with open(test_path, 'w') as test_f, open(gt_path, 'w') as gt_f:\n",
    "            for idx, sent in enumerate(test_sents, 1):\n",
    "                test_entry = {\n",
    "                    'id': f'{ontology_id}_test_{idx}',\n",
    "                    'sent': sent['sent']\n",
    "                }\n",
    "                gt_entry = {\n",
    "                    'id': f'{ontology_id}_test_{idx}',\n",
    "                    'sent': sent['sent'],\n",
    "                    'triples': sent['triples']\n",
    "                }\n",
    "                test_f.write(json.dumps(test_entry) + '\\n')\n",
    "                gt_f.write(json.dumps(gt_entry) + '\\n')\n",
    "        \n",
    "        print(f\"  Saved to:\")\n",
    "        print(f\"  - {train_path}\")\n",
    "        print(f\"  - {test_path}\")\n",
    "        print(f\"  - {gt_path}\")\n",
    "    \n",
    "    print(\"\\nFinal statistics:\")\n",
    "    print(f\"Total sentences processed: {total_all_sents}\")\n",
    "    print(f\"Total train sentences: {total_train}\")\n",
    "    print(f\"Total test sentences: {total_test}\")\n",
    "    print(f\"Total ontologies processed: {len(ontology_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Odeuropa dataset processing...\n",
      "Loading ontologies...\n",
      "Loaded Smell Emission Ontology (ont_l12_smell_emission)\n",
      "Loaded Olfactory Experience Ontology (ont_l13_olfactory_experience)\n",
      "\n",
      "Processing TSV file: ../../odeuropa_subset_british-library_1890-1899/BritishLibrary-1890_1899-frames.tsv\n",
      "Processed 1000 TSV rows\n",
      "\n",
      "Splitting data and saving files:\n",
      "\n",
      "ont_l12_smell_emission:\n",
      "  Total sentences: 742\n",
      "  Train sentences: 557\n",
      "  Test sentences: 185\n",
      "  Unique subjects: 889\n",
      "  Saved to:\n",
      "  - ./data/odeuropa/train/ont_l12_smell_emission_train.jsonl\n",
      "  - ./data/odeuropa/test/ont_l12_smell_emission_test.jsonl\n",
      "  - ./data/odeuropa/ground_truth/ont_l12_smell_emission_ground_truth.jsonl\n",
      "\n",
      "ont_l13_olfactory_experience:\n",
      "  Total sentences: 583\n",
      "  Train sentences: 450\n",
      "  Test sentences: 133\n",
      "  Unique subjects: 297\n",
      "  Saved to:\n",
      "  - ./data/odeuropa/train/ont_l13_olfactory_experience_train.jsonl\n",
      "  - ./data/odeuropa/test/ont_l13_olfactory_experience_test.jsonl\n",
      "  - ./data/odeuropa/ground_truth/ont_l13_olfactory_experience_ground_truth.jsonl\n",
      "\n",
      "Final statistics:\n",
      "Total sentences processed: 1325\n",
      "Total train sentences: 1007\n",
      "Total test sentences: 318\n",
      "Total ontologies processed: 2\n",
      "\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "ontology_dir = './data/odeuropa/ontologies'\n",
    "tsv_path = '../../odeuropa_subset_british-library_1890-1899/BritishLibrary-1890_1899-frames.tsv'\n",
    "\n",
    "# Load ontologies\n",
    "print(\"Starting Odeuropa dataset processing...\")\n",
    "ontologies = load_ontologies(ontology_dir)\n",
    "\n",
    "# Process TSV and generate ontology-specific data\n",
    "ontology_data = process_tsv(tsv_path, ontologies)\n",
    "\n",
    "# Split data and save\n",
    "split_and_save_data(ontology_data)\n",
    "print(\"\\nProcessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
