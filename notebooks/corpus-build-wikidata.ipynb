{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204faf9-d1b1-475f-890b-519ac0051d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import http.client\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from SPARQLWrapper.SPARQLExceptions import SPARQLWrapperException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b6466-2d46-4f27-b0ce-e14530a8cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d24a2-dc53-444b-8aeb-887afa3150f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_date_string(date_string):\n",
    "    pattern = r\"^(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})Z$\"\n",
    "    match = re.match(pattern, date_string)\n",
    "    if match:\n",
    "        year, month, day, hour, minute, second = match.groups()\n",
    "        date = datetime(int(year), int(month), int(day))\n",
    "        month_name = date.strftime(\"%B\")\n",
    "        new_date_string = f\"{day} {month_name} {year}\"\n",
    "        return new_date_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_splits(triples, splits = [0.4, 0.3, 0.3]):\n",
    "    triples = np.array(triples)\n",
    "    indices = np.random.permutation(triples.shape[0])\n",
    "    train_count = int(triples.shape[0] * splits[0])\n",
    "    val_count = int(triples.shape[0] * splits[1])\n",
    "    test_count = triples.shape[0] - train_count - val_count\n",
    "    train_triples = triples[indices[:train_count]]\n",
    "    val_triples = triples[indices[train_count:train_count+val_count]]\n",
    "    test_triples = triples[indices[train_count+val_count:]]\n",
    "    return train_triples.tolist(), val_triples.tolist(), test_triples.tolist()\n",
    "\n",
    "def save_triples(onto_id, train_all, val_all, test_all):\n",
    "    # Define base paths for different directories\n",
    "    base_paths = {\n",
    "        'train': \"./data/wikidata_tekgen/train\",\n",
    "        'validation': \"./data/wikidata_tekgen/validation\",\n",
    "        'ground_truth': \"./data/wikidata_tekgen/ground_truth\",\n",
    "        'test': \"./data/wikidata_tekgen/test\"\n",
    "    }\n",
    "\n",
    "    # Ensure all required directories exist\n",
    "    for path in base_paths.values():\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save train data\n",
    "    with open(f\"{base_paths['train']}/{onto_id}_train.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(train_all):\n",
    "            data = {\"id\": f\"{onto_id}_train_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2],\n",
    "                    \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Save validation data\n",
    "    with open(f\"{base_paths['validation']}/{onto_id}_validation.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(val_all):\n",
    "            data = {\"id\": f\"{onto_id}_val_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2],\n",
    "                    \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Group test triples by sentences and remove duplicate triples\n",
    "    sentence_to_triples = defaultdict(set)\n",
    "    for tr in test_all:\n",
    "        sent = tr[6]\n",
    "        triple_tuple = (tr[0], tr[1], tr[2])  # Using labels: sub_label, rel_label, obj_label\n",
    "        sentence_to_triples[sent].add(triple_tuple)\n",
    "\n",
    "    # Save ground truth data\n",
    "    with open(f\"{base_paths['ground_truth']}/{onto_id}_ground_truth.jsonl\", \"w\") as out_file:\n",
    "        for idx, (sent, triples_set) in enumerate(sentence_to_triples.items()):\n",
    "            # Convert set of tuples back to list of dicts\n",
    "            triples = [{\"sub\": sub, \"rel\": rel, \"obj\": obj} for (sub, rel, obj) in triples_set]\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": sent, \"triples\": triples}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Save test data\n",
    "    with open(f\"{base_paths['test']}/{onto_id}_test.jsonl\", \"w\") as out_file:\n",
    "        for idx, (sent, _) in enumerate(sentence_to_triples.items()):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": sent}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "def get_triples_with_sentences(relation_pid: str, relation_label: str, rel_domain: str, rel_range: str,\n",
    "                               limit: int = 200, max_retries: int = 10):\n",
    "    assert relation_pid, \"relation id can't be empty\"\n",
    "    assert rel_domain, \"domain can't be empty\"\n",
    "\n",
    "    current_limit = 10000  # Start with a high limit for SPARQL query\n",
    "    retries = 0\n",
    "    # Set the User-Agent according to Wikidata's policy\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Build the SPARQL query\n",
    "            sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "            query = \"PREFIX wdt: <http://www.wikidata.org/prop/direct/> \\n PREFIX wd: <http://www.wikidata.org/entity/> \\n\"\n",
    "            query += \"SELECT DISTINCT ?sub ?subEntity ?objEntity ?objLabel { \\n ?subEntity wdt:P31/wdt:P279* wd:\" + rel_domain + \" . \\n\"\n",
    "            query += '?subEntity rdfs:label ?sub . FILTER (lang(?sub) = \"en\") \\n '\n",
    "            query += '?subEntity wdt:' + relation_pid + ' ?objEntity . \\n'\n",
    "            if rel_range and rel_range != \"\":\n",
    "                query += '?objEntity wdt:P31*/wdt:P279* wd:' + rel_range + ' . \\n '\n",
    "            query += 'OPTIONAL { ?objEntity rdfs:label ?objLabel . FILTER (lang(?objLabel) = \"en\") } \\n } '\n",
    "            # Set the dynamic LIMIT\n",
    "            query += f\"LIMIT {current_limit}\"\n",
    "            if show_query:\n",
    "                print(query)\n",
    "\n",
    "            # Generate a hash of the query string to use as a cache filename\n",
    "            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()\n",
    "            cache_directory = 'sparql_cache'\n",
    "            cache_filename = os.path.join(cache_directory, f'{query_hash}.json')\n",
    "\n",
    "            # Ensure the cache directory exists\n",
    "            os.makedirs(cache_directory, exist_ok=True)\n",
    "\n",
    "            # Check if the cache file exists\n",
    "            if os.path.exists(cache_filename):\n",
    "                # Load the cached results from the file\n",
    "                with open(cache_filename, 'r', encoding='utf-8') as cache_file:\n",
    "                    cached_data = json.load(cache_file)\n",
    "                    triples = cached_data['triples']\n",
    "                    print(\"Loaded results from cache.\")\n",
    "                # Store in the in-memory cache as well\n",
    "                sparql_query_cache[query] = triples\n",
    "            elif query in sparql_query_cache:\n",
    "                # Use cached results from in-memory cache\n",
    "                triples = sparql_query_cache[query]\n",
    "            else:\n",
    "                # Execute the query and get a set of triples\n",
    "                triples = list()\n",
    "                subject_counter, object_counter = Counter(), Counter()\n",
    "                secondary_triples = list()\n",
    "                sparql.setQuery(query)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                sparql.setTimeout(300)  # Set timeout to 5 minutes\n",
    "                sparql.setMethod('POST')\n",
    "\n",
    "                # Set the User-Agent\n",
    "                sparql.agent = user_agent\n",
    "\n",
    "                # Attempt to execute the query\n",
    "                response = sparql.query()\n",
    "                results = response.convert()\n",
    "\n",
    "                print(f'  {len(results[\"results\"][\"bindings\"])} SPARQL results.')\n",
    "                for result in results[\"results\"][\"bindings\"]:\n",
    "                    t_subject = result['sub']['value']\n",
    "                    if 'objLabel' in result:\n",
    "                        t_object = result['objLabel']['value']\n",
    "                        t_object_id = result['objEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    else:\n",
    "                        t_object = result['objEntity']['value']\n",
    "                        date_string = convert_date_string(t_object)\n",
    "                        if date_string:\n",
    "                            t_object = date_string\n",
    "                        t_object_id = None\n",
    "                    t_subject_id = result['subEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    triple = [t_subject, relation_label, t_object, t_subject_id, relation_pid, t_object_id]\n",
    "                    # To get a diverse dataset, ignore subject/object if they occur more than 10% of the limit\n",
    "                    subject_counter[t_subject] += 1\n",
    "                    object_counter[t_object] += 1\n",
    "                    if subject_counter[t_subject] > (limit / 10) or object_counter[t_object] > (limit / 10):\n",
    "                        secondary_triples.append(triple)\n",
    "                        continue\n",
    "                    triples.append(triple)\n",
    "\n",
    "                # Append secondary triples\n",
    "                triples += secondary_triples\n",
    "                # Save to in-memory cache\n",
    "                sparql_query_cache[query] = triples\n",
    "                # Save results to cache file\n",
    "                with open(cache_filename, 'w', encoding='utf-8') as cache_file:\n",
    "                    json.dump({'triples': triples}, cache_file)\n",
    "                print(\"Saved results to cache.\")\n",
    "\n",
    "            print(f\"  collected {len(triples)} triples\")\n",
    "            if show_sample:\n",
    "                print(f\"  sample:\")\n",
    "                for tr in triples[:5]:\n",
    "                    print(f\"    {tr[:3]}\")\n",
    "\n",
    "            triples_with_sentences = list()\n",
    "            for tr in triples:\n",
    "                search_key = create_key(tr[0], tr[1], tr[2])\n",
    "                if search_key in sent_index:\n",
    "                    sentence = sent_index[search_key]\n",
    "                else:\n",
    "                    continue\n",
    "                tr.append(sentence)\n",
    "                triples_with_sentences.append(tr)\n",
    "\n",
    "                # Stop at the desired limit\n",
    "                if len(triples_with_sentences) >= limit:\n",
    "                    break\n",
    "\n",
    "            # If successful, return the collected triples with sentences\n",
    "            return triples_with_sentences\n",
    "\n",
    "        except SPARQLWrapperException as e:\n",
    "            retries += 1\n",
    "            code = None\n",
    "            reason = str(e)\n",
    "\n",
    "            # Try to parse the HTTP status code from the exception message\n",
    "            match = re.search(r'status code (\\d+)', reason, re.IGNORECASE)\n",
    "            if match:\n",
    "                code = int(match.group(1))\n",
    "\n",
    "            if code == 429:\n",
    "                # HTTP 429: Too Many Requests\n",
    "                retry_after = '60'  # Default wait time\n",
    "                wait_time = int(retry_after)\n",
    "                print(f\"HTTP 429 error encountered. Waiting for {wait_time} seconds before retrying ({retries}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif code == 500:\n",
    "                print(f\"HTTP 500 error encountered on attempt {retries}/{max_retries}. Reducing LIMIT and retrying...\")\n",
    "                current_limit = max(10, current_limit // 2)\n",
    "                #time.sleep(5)\n",
    "            else:\n",
    "                print(f\"HTTP error {code} encountered: {reason}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "                time.sleep(5)\n",
    "        except (http.client.IncompleteRead, json.JSONDecodeError) as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            # Reduce the LIMIT and retry\n",
    "            current_limit = max(10, current_limit // 2)\n",
    "            print(f\"Reducing LIMIT to {current_limit} and retrying...\")\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            #time.sleep(5)\n",
    "\n",
    "    print(\"Max retries reached. Skipping this relation.\")\n",
    "    return []\n",
    "\n",
    "def create_key(sub_label, rel_label, obj_label):\n",
    "    # remove spaces and make lower case\n",
    "    sub_label = re.sub(r\"\\s+\", '', sub_label).lower()\n",
    "    rel_label = re.sub(r\"\\s+\", '', rel_label).lower()\n",
    "    obj_label = re.sub(r\"\\s+\", '', obj_label).lower()\n",
    "    # concatanate them\n",
    "    tr_key = f\"{sub_label}{rel_label}{obj_label}\"\n",
    "    return tr_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477d7d4-c1f3-4123-b2a4-cd28ba55a03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TekGen corpus\n",
    "sent_index = dict()\n",
    "start_time = time.time()\n",
    "print(\"TekGen corpus processing started!\")\n",
    "with open('../../tekgen.csv') as csv_in_file:\n",
    "    sent_reader = csv.reader(csv_in_file)\n",
    "    next(sent_reader)\n",
    "    for row in sent_reader:\n",
    "        tr_key = create_key(row[0], row[1], row[2])\n",
    "        sent = row[4]\n",
    "        sent_index[tr_key] = sent\n",
    "        elapsed_time = (time.time()-start_time)/60\n",
    "    print(f\"\\ttriple-to-sent index with {len(sent_index)} triples loaded in {elapsed_time:.2f} mins!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c801336-be11-4395-83eb-d889b334289e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update base path to use existing ontology directory\n",
    "base_path = './data/wikidata_tekgen/ontologies'\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Check if directory is empty or missing ontology files\n",
    "ontology_files = [f for f in os.listdir(base_path) if f.endswith('_ontology.json')]\n",
    "if not ontology_files:\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load existing ontologies\n",
    "ontologies = []\n",
    "for filename in os.listdir(base_path):\n",
    "    if filename.endswith('_ontology.json'):\n",
    "        with open(os.path.join(base_path, filename)) as in_file:\n",
    "            ontologies.append(json.load(in_file))\n",
    "\n",
    "# Main flags\n",
    "show_sample = True\n",
    "show_query = False\n",
    "\n",
    "for onto in ontologies:\n",
    "    print(f\"Ontology: {onto['title']} ({onto['id']})\")\n",
    "    onto_id = onto['id']\n",
    "\n",
    "    # Reset accumulators for each ontology\n",
    "    train_all, val_all, test_all = [], [], []\n",
    "\n",
    "    for rel in onto['relations']:\n",
    "        print(f\"\\nprocessing \\\"{rel['label']}\\\" ({rel['pid']}) relation:\")\n",
    "        start_time = time.time()\n",
    "        triples_with_sentences = get_triples_with_sentences(\n",
    "            rel['pid'], rel['label'], rel['domain'], rel['range'], limit=200, max_retries=10)\n",
    "        elapsed_time = (time.time() - start_time)\n",
    "        print(f\"    {len(triples_with_sentences)} triples with sentences in {elapsed_time:.2f} seconds!\")\n",
    "        train, val, test = get_splits(triples_with_sentences)\n",
    "        train_all += train\n",
    "        val_all += val\n",
    "        test_all += test\n",
    "\n",
    "    # Save triples after processing each ontology\n",
    "    save_triples(onto_id, train_all, val_all, test_all)\n",
    "    print(f\"Finished processing ontology {onto_id}. Data saved.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
